{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tisya Sharma 002903942\n",
    "\n",
    "Professor Miguel Fuentes-Cabrera\n",
    "\n",
    "15 October, 2025\n",
    "\n",
    "DS4400: Machine Learning and Data Mining\n",
    "\n",
    "Homework 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy empirical X: 17.2\n",
      "NumPy population X: 13.76\n",
      "NumPy empirical Y: 5.7\n",
      "NumPy population Y: 4.56\n"
     ]
    }
   ],
   "source": [
    "X = np.array([4, 7, 9, 11, 15], dtype=float)\n",
    "Y = np.array([-26, -25, -23, -22, -20], dtype=float)\n",
    "\n",
    "print(\"NumPy empirical X:\", round(float(np.var(X, ddof=1)), 6))\n",
    "print(\"NumPy population X:\", round(float(np.var(X, ddof=0)), 6))\n",
    "print(\"NumPy empirical Y:\", round(float(np.var(Y, ddof=1)), 6))\n",
    "print(\"NumPy population Y:\", round(float(np.var(Y, ddof=0)), 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 364 iterations\n",
      "Final loss: 0.246807262619\n",
      "Final w:\n",
      "w1 = -0.221556582\n",
      "w2 = -0.684796388\n",
      "w3 = 0.801512867\n",
      "w4 = 0.217913067\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression with gradient descent\n",
    "\n",
    "lam = 0.2\n",
    "X = np.array([[1,-2,1,0],\n",
    "              [-2,-1,2,1],\n",
    "              [1, 2,1,-1]], dtype=float)\n",
    "y = np.array([[2],[3],[-1]], dtype=float)\n",
    "w0 = np.array([[-3],[0],[2],[1]], dtype=float)\n",
    "\n",
    "def ridge_loss(w):\n",
    "    r = np.dot(X, w) - y\n",
    "    return float(np.sum(r**2) + lam*np.sum(w**2))\n",
    "\n",
    "def grad_ridge(w):\n",
    "    return 2*np.dot(X.T, (np.dot(X, w) - y)) + 2*lam*w\n",
    "\n",
    "# picked an lr that converges\n",
    "lr = 0.05\n",
    "max_iter = 100000\n",
    "tol = 1e-10\n",
    "\n",
    "w = w0.copy()\n",
    "prev = 1e18\n",
    "for t in range(1, max_iter+1):\n",
    "    L = ridge_loss(w)\n",
    "    if abs(prev - L) < tol:\n",
    "        break\n",
    "    prev = L\n",
    "    w = w - lr * grad_ridge(w)\n",
    "\n",
    "print(\"Converged in\", t, \"iterations\")\n",
    "print(\"Final loss:\", round(float(L), 12))\n",
    "print(\"Final w:\")\n",
    "for i, name in enumerate([\"w1\",\"w2\",\"w3\",\"w4\"]):\n",
    "    print(name, \"=\", round(float(w[i,0]), 9))\n",
    "\n",
    "w_gd = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank(X^T X) = 3  out of 4\n",
      "Ridge closed-form w:\n",
      "w1 = -0.22151398\n",
      "w2 = -0.684772044\n",
      "w3 = 0.801518953\n",
      "w4 = 0.218010443\n",
      "||w_ridge_cf - w_gd||_2 = 0.000109209092\n"
     ]
    }
   ],
   "source": [
    "# check singularity of X^T X\n",
    "XT_X = np.dot(X.T, X)\n",
    "print(\"rank(X^T X) =\", np.linalg.matrix_rank(XT_X), \" out of 4\")\n",
    "\n",
    "# ridge closed form \n",
    "w_cf_ridge = np.dot(np.linalg.inv(XT_X + lam*np.eye(X.shape[1])), np.dot(X.T, y))\n",
    "print(\"Ridge closed-form w:\")\n",
    "for i, name in enumerate([\"w1\",\"w2\",\"w3\",\"w4\"]):\n",
    "    print(name, \"=\", round(float(w_cf_ridge[i,0]), 9))\n",
    "\n",
    "# compare to GD\n",
    "diff = np.linalg.norm(w_cf_ridge - w_gd)\n",
    "print(\"||w_ridge_cf - w_gd||_2 =\", round(float(diff), 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plain OLS formula fails because (X^T X) can’t be inverted since it is singular. That happens here since there are more weights than data points (p>n), so (X^T X) is not full rank. Ridge fixes this by adding (lambda I), which makes the matrix invertible, gives a unique and stable solution, and helps prevent overfitting, especially when features outnumber samples or are highly collinear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num cols: ['Sodium', 'Calcium', 'Potassium'] | cat cols: ['Recipe'] | one-hot dims: 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import autograd.numpy as np\n",
    "\n",
    "# read\n",
    "df = pd.read_csv(\"recipes_df.csv\")\n",
    "\n",
    "# target and features\n",
    "y = df[\"Protein\"].astype(float)\n",
    "X_df = df.drop(columns=[\"Protein\"])\n",
    "\n",
    "# split by dtype (only 'Recipe' is categorical here)\n",
    "cat_cols = [\"Recipe\"] if \"Recipe\" in X_df.columns else []\n",
    "num_cols = [c for c in X_df.columns if c not in cat_cols]  # ['Sodium','Calcium','Potassium'] in your file\n",
    "\n",
    "# one-hot encode categoricals (0/1) and keep numeric raw (no scaling here)\n",
    "X_cats = pd.get_dummies(X_df[cat_cols], drop_first=True, dtype=int) if len(cat_cols) else pd.DataFrame(index=X_df.index)\n",
    "X_num  = X_df[num_cols].copy()\n",
    "\n",
    "print(\"num cols:\", num_cols, \"| cat cols:\", cat_cols, \"| one-hot dims:\", X_cats.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3 (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi shapes: (20, 11) (7, 11) (7, 11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "# 60/20/20 split\n",
    "Xn_tmp, Xn_test, y_tmp, y_test, Xc_tmp, Xc_test = train_test_split(\n",
    "    X_num, y, X_cats, test_size=0.20, random_state=0\n",
    ")\n",
    "Xn_train, Xn_val, y_train_s, y_val_s, Xc_train, Xc_val = train_test_split(\n",
    "    Xn_tmp, y_tmp, Xc_tmp, test_size=0.25, random_state=0\n",
    ")\n",
    "\n",
    "# scalers / poly on train only\n",
    "num_scaler = StandardScaler().fit(Xn_train.values)\n",
    "poly2 = PolynomialFeatures(degree=2, include_bias=False).fit(num_scaler.transform(Xn_train.values))\n",
    "\n",
    "def make_Phi(numeric_df, cats_df):\n",
    "    A = num_scaler.transform(numeric_df.values)      # scaled numeric\n",
    "    E = poly2.transform(A)                           # [num, num^2, crosses]  (NO bias here)\n",
    "    D = cats_df.values.astype(float) if cats_df.shape[1] else np.zeros((len(cats_df), 0))\n",
    "    M = np.column_stack([E, D])                      # concat\n",
    "    Phi = pd.DataFrame(M)\n",
    "    Phi.insert(0, \"intercept\", 1.0)                  # single explicit intercept\n",
    "    return Phi\n",
    "\n",
    "Phi_train = make_Phi(Xn_train, Xc_train)\n",
    "Phi_val   = make_Phi(Xn_val,   Xc_val)\n",
    "Phi_test  = make_Phi(Xn_test,  Xc_test)\n",
    "\n",
    "# y to column vectors\n",
    "y_train = y_train_s.values.reshape(-1,1).astype(float)\n",
    "y_val   = y_val_s.values.reshape(-1,1).astype(float)\n",
    "y_test  = y_test.values.reshape(-1,1).astype(float)\n",
    "\n",
    "print(\"Phi shapes:\", Phi_train.shape, Phi_val.shape, Phi_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3 (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso (GD): best λ = 1.0  | val MSE = 139.677923\n",
      "Ridge (GD): best λ = 0.1  | val MSE = 113.414032\n",
      "Elastic (GD): best λ = 0.1  | l1_ratio = 0.7  | val MSE = 63.232482\n",
      "Lasso (sklearn) val MSE: 27.388184\n",
      "Ridge  (sklearn) val MSE: 56.481691\n",
      "Elastic(sklearn) val MSE: 32.055884\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def mse(Phi, y, w):\n",
    "    r = np.dot(Phi, w) - y\n",
    "    return float(np.mean(r**2))\n",
    "\n",
    "# mask so we do not regularize the intercept (first column)\n",
    "no_penalty = np.ones((Phi_train.shape[1], 1))\n",
    "no_penalty[0,0] = 0.0\n",
    "\n",
    "# simple GD solvers\n",
    "def gd_ridge(Phi, y, lam, lr=0.02, niter=8000, tol=1e-8):\n",
    "    n, p = Phi.shape\n",
    "    w = np.zeros((p,1))\n",
    "    for t in range(niter):\n",
    "        r = np.dot(Phi, w) - y\n",
    "        grad = (2.0/n) * np.dot(Phi.T, r)\n",
    "        grad[0,0] = 0.0\n",
    "        grad += 2.0 * lam * np.vstack([[[0.0]], w[1:]])\n",
    "        w_new = w - lr * grad\n",
    "        if not np.isfinite(w_new).all(): lr *= 0.5; continue\n",
    "        if np.linalg.norm(w_new - w) < tol: w = w_new; break\n",
    "        w = w_new\n",
    "    return w\n",
    "\n",
    "def gd_lasso(Phi, y, lam, lr=0.02, niter=20000, tol=1e-8):\n",
    "    n, p = Phi.shape\n",
    "    w = np.zeros((p,1))\n",
    "    for t in range(niter):\n",
    "        r = np.dot(Phi, w) - y\n",
    "        grad = (2.0/n) * np.dot(Phi.T, r)\n",
    "        grad[0,0] = 0.0\n",
    "        w_mid = w - lr * grad\n",
    "        w_new = w_mid.copy()\n",
    "        w_new[1:] = np.sign(w_mid[1:]) * np.maximum(np.abs(w_mid[1:]) - lr*lam, 0.0)\n",
    "        if not np.isfinite(w_new).all(): lr *= 0.5; continue\n",
    "        if np.linalg.norm(w_new - w) < tol: w = w_new; break\n",
    "        w = w_new\n",
    "    return w\n",
    "\n",
    "def gd_elastic(Phi, y, lam, l1_ratio=0.5, lr=0.02, niter=20000, tol=1e-8):\n",
    "    n, p = Phi.shape\n",
    "    a = l1_ratio; b = 1.0 - a\n",
    "    w = np.zeros((p,1))\n",
    "    for t in range(niter):\n",
    "        r = np.dot(Phi, w) - y\n",
    "        grad = (2.0/n) * np.dot(Phi.T, r)\n",
    "        grad[0,0] = 0.0\n",
    "        grad += 2.0 * lam * b * np.vstack([[[0.0]], w[1:]])        # ridge part\n",
    "        w_mid = w - lr * grad\n",
    "        thr = lr * lam * a\n",
    "        w_new = w_mid.copy()\n",
    "        w_new[1:] = np.sign(w_mid[1:]) * np.maximum(np.abs(w_mid[1:]) - thr, 0.0)\n",
    "        if not np.isfinite(w_new).all(): lr *= 0.5; continue\n",
    "        if np.linalg.norm(w_new - w) < tol: w = w_new; break\n",
    "        w = w_new\n",
    "    return w\n",
    "\n",
    "# grid of lambdas\n",
    "lambda_list_l1 = [0.01, 0.1, 1.0, 10.0]      # for lasso/elastic\n",
    "lambda_list_l2 = [0.0, 0.01, 0.1, 1.0, 10.0] # for ridge\n",
    "\n",
    "# Lasso\n",
    "best_lam_lasso, best_val_lasso, best_w_lasso = None, 1e18, None\n",
    "for lam in lambda_list_l1:\n",
    "    w = gd_lasso(Phi_train.values, y_train, lam, lr=0.02, niter=20000)\n",
    "    v = mse(Phi_val.values, y_val, w)\n",
    "    if v < best_val_lasso: best_val_lasso, best_lam_lasso, best_w_lasso = v, lam, w\n",
    "print(\"Lasso (GD): best λ =\", best_lam_lasso, \" | val MSE =\", round(best_val_lasso, 6))\n",
    "\n",
    "# Ridge\n",
    "best_lam_ridge, best_val_ridge, best_w_ridge = None, 1e18, None\n",
    "for lam in lambda_list_l2:\n",
    "    w = gd_ridge(Phi_train.values, y_train, lam, lr=0.02, niter=8000)\n",
    "    v = mse(Phi_val.values, y_val, w)\n",
    "    if v < best_val_ridge: best_val_ridge, best_lam_ridge, best_w_ridge = v, lam, w\n",
    "print(\"Ridge (GD): best λ =\", best_lam_ridge, \" | val MSE =\", round(best_val_ridge, 6))\n",
    "\n",
    "# Elastic Net\n",
    "best_lam_en, best_ratio_en, best_val_en, best_w_en = None, None, 1e18, None\n",
    "for ratio in [0.3, 0.5, 0.7]:\n",
    "    for lam in lambda_list_l1:\n",
    "        w = gd_elastic(Phi_train.values, y_train, lam, l1_ratio=ratio, lr=0.02, niter=20000)\n",
    "        v = mse(Phi_val.values, y_val, w)\n",
    "        if v < best_val_en: best_val_en, best_lam_en, best_ratio_en, best_w_en = v, lam, ratio, w\n",
    "print(\"Elastic (GD): best λ =\", best_lam_en, \" | l1_ratio =\", best_ratio_en, \" | val MSE =\", round(best_val_en, 6))\n",
    "\n",
    "# quick sklearn comparison \n",
    "if best_lam_lasso is not None:\n",
    "    sk_lasso = Lasso(alpha=best_lam_lasso, fit_intercept=False, max_iter=10000)\n",
    "    sk_lasso.fit(Phi_train.values, y_train.ravel())\n",
    "    print(\"Lasso (sklearn) val MSE:\", round(mean_squared_error(y_val.ravel(), sk_lasso.predict(Phi_val.values)), 6))\n",
    "\n",
    "sk_ridge = Ridge(alpha=best_lam_ridge, fit_intercept=False)\n",
    "sk_ridge.fit(Phi_train.values, y_train.ravel())\n",
    "print(\"Ridge  (sklearn) val MSE:\", round(mean_squared_error(y_val.ravel(), sk_ridge.predict(Phi_val.values)), 6))\n",
    "\n",
    "sk_en = ElasticNet(alpha=best_lam_en, l1_ratio=best_ratio_en, fit_intercept=False, max_iter=10000)\n",
    "sk_en.fit(Phi_train.values, y_train.ravel())\n",
    "print(\"Elastic(sklearn) val MSE:\", round(mean_squared_error(y_val.ravel(), sk_en.predict(Phi_val.values)), 6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3 (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: elastic | val MSE: 63.232482\n",
      "Test MSE: 32937.470386 | R^2: -65.047473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vw/c11105x93rdfqx32_1qmzfn40000gn/T/ipykernel_79488/2079171815.py:51: RuntimeWarning: invalid value encountered in multiply\n",
      "  w_mid = w - lr * grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model: elastic | λ = 0.1 | l1_ratio = 0.7\n",
      "Number of weights: 11\n",
      "First 10 weights:\n",
      "w 0 = 0.0\n",
      "w 1 = 7.862561676340847e+303\n",
      "w 2 = 1.269478444438594e+301\n",
      "w 3 = 2.2180115670081132e+303\n",
      "w 4 = 5.669348035942454e+304\n",
      "w 5 = -1.0284880703786672e+303\n",
      "w 6 = 1.4461760743770974e+304\n",
      "w 7 = 1.0693225253038282e+303\n",
      "w 8 = 5.122796753376923e+302\n",
      "w 9 = 4.692353899054133e+303\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "candidates = [\n",
    "    (\"lasso\",   best_val_lasso, best_lam_lasso, best_w_lasso, None),\n",
    "    (\"ridge\",   best_val_ridge, best_lam_ridge, best_w_ridge, None),\n",
    "    (\"elastic\", best_val_en,    best_lam_en,    best_w_en,    best_ratio_en),\n",
    "]\n",
    "candidates.sort(key=lambda z: z[1])\n",
    "winner, best_val_mse, lam_star, w_star, ratio_star = candidates[0]\n",
    "\n",
    "print(\"Best model:\", winner, \"| val MSE:\", round(best_val_mse, 6))\n",
    "\n",
    "# test performance\n",
    "yhat_test = np.dot(Phi_test.values, w_star)\n",
    "test_mse = float(np.mean((yhat_test - y_test)**2))\n",
    "test_r2  = float(r2_score(y_test.ravel(), yhat_test.ravel()))\n",
    "print(\"Test MSE:\", round(test_mse, 6), \"| R^2:\", round(test_r2, 6))\n",
    "\n",
    "# refit winner on full data\n",
    "Phi_full = pd.concat([Phi_train, Phi_val, Phi_test], axis=0)\n",
    "y_full   = np.vstack([y_train, y_val, y_test])\n",
    "\n",
    "if winner == \"ridge\":\n",
    "    w_final = gd_ridge(Phi_full.values, y_full, lam_star, lr=0.02, niter=12000)\n",
    "elif winner == \"lasso\":\n",
    "    w_final = gd_lasso(Phi_full.values, y_full, lam_star, lr=0.02, niter=25000)\n",
    "else:\n",
    "    w_final = gd_elastic(Phi_full.values, y_full, lam_star, l1_ratio=ratio_star, lr=0.02, niter=25000)\n",
    "\n",
    "assert np.isfinite(w_final).all()\n",
    "\n",
    "print(\"Final model:\", winner, \"| λ =\", lam_star, (\"| l1_ratio = \" + str(ratio_star) if winner==\"elastic\" else \"\"))\n",
    "print(\"Number of weights:\", w_final.shape[0])\n",
    "print(\"First 10 weights:\")\n",
    "for i in range(min(10, w_final.shape[0])):\n",
    "    print(\"w\", i, \"=\", round(float(w_final[i,0]), 6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
