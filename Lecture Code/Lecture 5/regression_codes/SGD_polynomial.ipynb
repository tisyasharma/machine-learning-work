{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnbfYO1C0vLn",
        "outputId": "95dbefa6-ab78-4c5e-91bb-117efe5c1867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the learning rate\n",
            "0.01\n",
            "Enter the number of iterations\n",
            "100\n",
            "SGD learned: y ≈ 2.1188 * x^2 + 2.0350 * x + 0.6568\n"
          ]
        }
      ],
      "source": [
        "# --- sklearn: Gradient Descent with SGDRegressor ---\n",
        "\n",
        "# For the second order polynomial fitting case\n",
        "\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "import numpy as np\n",
        "phi = np.array([[0,0,1],\n",
        "              [1,1,1],\n",
        "              [4,2,1],\n",
        "              [9,3,1],\n",
        "              [16,4,1]])\n",
        "\n",
        "y = np.array([[0],\n",
        "              [5],\n",
        "              [14],\n",
        "              [27],\n",
        "              [44]])\n",
        "\n",
        "# Use the existing phi and y from earlier cells.\n",
        "# phi columns are [x^2, x, 1]; sklearn will learn the intercept, so drop the bias column:\n",
        "X = phi[:, :2]                 # shape (n_samples, 2) -> [x^2, x]\n",
        "y_vec = y.flatten()              # sklearn expects 1D targets\n",
        "\n",
        "# Let's give the user the chance to enter the learning rate and the\n",
        "# number of iterations\n",
        "lr = float(input('Enter the learning rate\\n'))\n",
        "niter = int(input('Enter the number of iterations\\n'))\n",
        "\n",
        "# Create and fit an SGD regressor (squared loss = linear regression)\n",
        "sgd = SGDRegressor(\n",
        "    loss=\"squared_error\",\n",
        "    penalty=None,       # no L1/L2 to match your vanilla GD\n",
        "    alpha=0.0,          # no regularization strength\n",
        "    learning_rate=\"constant\",\n",
        "    eta0=lr,            # step size = your lr\n",
        "    max_iter=niter,\n",
        "    tol=None,           # run full niter\n",
        "    random_state=0      # fixes the random number generator seed so\n",
        "                        # that the results are reproducible.\n",
        ")\n",
        "\n",
        "sgd.fit(X, y_vec)\n",
        "\n",
        "# Coefficients and intercept\n",
        "coef_x2, coef_x = sgd.coef_\n",
        "intercept = sgd.intercept_[0]\n",
        "print(f\"SGD learned: y ≈ {coef_x2:.4f} * x^2 + {coef_x:.4f} * x + {intercept:.4f}\")\n",
        "\n",
        "# Predictions for your downstream plot (keeps your variable name)\n",
        "final_predictions = sgd.predict(X).reshape(-1, 1)\n"
      ]
    }
  ]
}