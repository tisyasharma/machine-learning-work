{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2202b074-6b53-48ed-bca3-c2b22f97a43c",
      "metadata": {
        "id": "2202b074-6b53-48ed-bca3-c2b22f97a43c",
        "outputId": "5d02302a-ddb8-437d-f8a9-79d35ef8c076"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(7.0)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np # Importing the package NumPy\n",
        "x = np.array([[4], [3]]) # defining a vector (an array in Python jargon).\n",
        "                         # This is a column vector with two rows\n",
        "\n",
        "#l1 norm\n",
        "np.linalg.norm(x, ord=1) # See the link in the Text cell below to understand what\n",
        "                         # this commands does"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WJX92NltuklF",
      "metadata": {
        "id": "WJX92NltuklF"
      },
      "source": [
        "[numpy.linalg.norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)\n",
        "\n",
        "This function is able to return one of eight different matrix norms, or one of an infinite number of vector norms (described below), depending on the value of the ord parameter.\n",
        "\n",
        "# Usage\n",
        "linalg.norm(x, ord=None, axis=None, keepdims=False)\n",
        "x, input array.\n",
        "\n",
        "ord, order of the norm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "923cfd90-7620-4207-b93f-cf9e3e0a2e97",
      "metadata": {
        "id": "923cfd90-7620-4207-b93f-cf9e3e0a2e97",
        "outputId": "e51f0a37-574b-49ae-ddac-32fe637e5bd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(5.0)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#l2 norm\n",
        "np.linalg.norm(x, ord=2) # Same as above but now calculating an L2 norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6b7f8582-b999-4c0a-a031-004f79b89b50",
      "metadata": {
        "id": "6b7f8582-b999-4c0a-a031-004f79b89b50",
        "outputId": "62a4aaa0-16f0-4834-852f-8b77ddc2e224"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(4.0)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#l-infinity norm\n",
        "np.linalg.norm(x, ord=np.inf) # Calculating the L infinity norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a31aa846-aa17-40fc-a4ff-6ff8f749228f",
      "metadata": {
        "id": "a31aa846-aa17-40fc-a4ff-6ff8f749228f",
        "outputId": "edac7704-75ee-4eb6-86c9-ee059c4ca39c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#l0 norm (different function) Tis note: Cardinality\n",
        "np.count_nonzero(x) # Notice, here we're using a different command. See description below"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jwRW8o6sv2B5",
      "metadata": {
        "id": "jwRW8o6sv2B5"
      },
      "source": [
        "[numpy.count_nonzero](https://numpy.org/doc/stable/reference/generated/numpy.count_nonzero.html)\n",
        "\n",
        "Counts the number of non-zero values in the input array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "33f451b1-3f94-4498-8e1d-34cb2ad54981",
      "metadata": {
        "id": "33f451b1-3f94-4498-8e1d-34cb2ad54981",
        "outputId": "9d4d8b3b-a9b9-44a1-d383-73617f3fb8aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[17, 12],\n",
              "       [17,  0],\n",
              "       [17, 15],\n",
              "       [16,  3],\n",
              "       [16,  0],\n",
              "       [17,  0],\n",
              "       [12,  5],\n",
              "       [16, 25],\n",
              "       [12,  8],\n",
              "       [17,  0],\n",
              "       [18,  0],\n",
              "       [17,  0],\n",
              "       [16,  5],\n",
              "       [18,  0],\n",
              "       [16, 12],\n",
              "       [17, 15],\n",
              "       [16,  0],\n",
              "       [16,  0],\n",
              "       [18, 10],\n",
              "       [16, 10],\n",
              "       [17, 10],\n",
              "       [16,  0],\n",
              "       [16, 20],\n",
              "       [17,  0],\n",
              "       [17, 20],\n",
              "       [16,  0],\n",
              "       [18,  0]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get the csv reading function\n",
        "from numpy import genfromtxt\n",
        "X = genfromtxt('numonly_gtky.csv', delimiter=',', dtype=int)\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d3028387-a91f-40dd-b5d8-04ab40f571c2",
      "metadata": {
        "id": "d3028387-a91f-40dd-b5d8-04ab40f571c2"
      },
      "outputs": [],
      "source": [
        "# after manipulation, you can save numpy arrays as csv files\n",
        "Y = X.T\n",
        "np.savetxt('test.csv', Y, delimiter=',', fmt = '%d') #can be %d or %f or %.3f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e4c5b896-0420-442b-ac4f-e3035993d568",
      "metadata": {
        "id": "e4c5b896-0420-442b-ac4f-e3035993d568"
      },
      "outputs": [],
      "source": [
        "import autograd.numpy as np\n",
        "from autograd import grad\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8d816eb0-7b68-4709-aed0-ed534d88efb3",
      "metadata": {
        "id": "8d816eb0-7b68-4709-aed0-ed534d88efb3"
      },
      "outputs": [],
      "source": [
        "# building the x and w vectors for a single observation (regression objective function)\n",
        "x = np.array([[2],[1]]) # inputs (features), represents your independent variable(s)\n",
        "w = np.array([[1],[1]]) # the weight vector (parameters to learn), these are the coefficients that the model will adjust to minimize error.\n",
        "y = 1 # target output (true label), this is the dependent variable — the “true” value your model is trying to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2079c3bb-f0a5-44b3-a142-2bb37658fabc",
      "metadata": {
        "id": "2079c3bb-f0a5-44b3-a142-2bb37658fabc"
      },
      "outputs": [],
      "source": [
        "# setting n = 1; you'll need to update for different value of n\n",
        "# this is a regression objective function (loss function)\n",
        "# this is computing squared error loss — the difference between the predicted output, and the true output y.\n",
        "# we try to MINIMIZE the objective/loss function. \n",
        "\n",
        "def f(w):\n",
        "\treturn (np.dot(w.T,x) - y)**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9a1a456b-3769-48f7-aba2-bb6f9ca6fbbd",
      "metadata": {
        "id": "9a1a456b-3769-48f7-aba2-bb6f9ca6fbbd"
      },
      "outputs": [],
      "source": [
        "# derivative wrt w of the regression objective function\n",
        "# this is also known as the GRADIENT, the derivative of the loss function is the gradient\n",
        "# it gives you the gradient of the loss with respect to w, and you use this gradient to update w during gradient descent\n",
        "# the final goal of this regression is to minimize loss function (f(w)) and learn the optimal weights (w)\n",
        "def dfd(w):\n",
        "\treturn 2*(w.T.dot(x) - y)*x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ce1cf8d3-ab66-4801-b5de-a4acbee10f60",
      "metadata": {
        "id": "ce1cf8d3-ab66-4801-b5de-a4acbee10f60",
        "outputId": "13f4e5a9-bb02-4883-e768-8b0d6ee7fcf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autogen Gradient : \n",
            " [[8]\n",
            " [4]]\n",
            "Theoretical Gradient : \n",
            " [[8]\n",
            " [4]]\n"
          ]
        }
      ],
      "source": [
        "grad_foo = grad(f)       # Obtain its gradient function, autograd computes df/dw automatically\n",
        "print('Autogen Gradient : \\n', grad_foo(w))\n",
        "print('Theoretical Gradient : \\n', dfd(w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a4963633-6ef6-4c6c-bd07-dd45337ddf47",
      "metadata": {
        "id": "a4963633-6ef6-4c6c-bd07-dd45337ddf47"
      },
      "outputs": [],
      "source": [
        "# trace of a quadratic\n",
        "n = 3\n",
        "A = np.random.random((n,n))\n",
        "x = np.random.random((n,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5e12f67f-7aae-4a5e-830d-0cb0c1338b92",
      "metadata": {
        "id": "5e12f67f-7aae-4a5e-830d-0cb0c1338b92",
        "outputId": "a24fa9ed-423d-416b-d63c-5a0316e5212c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quadratic Autogen Gradient: \n",
            " [[1.82593449]\n",
            " [1.84990888]\n",
            " [0.41076563]]\n",
            "Quadratic Theoretical Gradient: \n",
            " [[1.82593449]\n",
            " [1.84990888]\n",
            " [0.41076563]]\n"
          ]
        }
      ],
      "source": [
        "# the function\n",
        "def f(x, A):\n",
        "    return np.trace(np.dot(np.dot(np.transpose(x), A), x))\n",
        "\n",
        "# the theoretical derivative (see lecture notes)\n",
        "def dfd(x, A):\n",
        "    return np.dot((np.transpose(A) + A), x)\n",
        "\n",
        "# the autograd derivative\n",
        "grad_foo = grad(f)\n",
        "\n",
        "print('Quadratic Autogen Gradient: \\n', grad_foo(x, A))\n",
        "print('Quadratic Theoretical Gradient: \\n', dfd(x, A))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "98aa8a19-cba5-424f-a656-3819fb06bbf8",
      "metadata": {
        "id": "98aa8a19-cba5-424f-a656-3819fb06bbf8",
        "outputId": "b7b4f43e-f899-4ffd-d0c0-a094432444de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multivariate Gaussian Autogen Grad: \n",
            " [[-0.3967452 ]\n",
            " [-0.40195444]\n",
            " [-0.08925254]]\n",
            "Multivariate Gaussian Theoretical Grad: \n",
            " [[-0.3967452 ]\n",
            " [-0.40195444]\n",
            " [-0.08925254]]\n"
          ]
        }
      ],
      "source": [
        "# multivariate gaussian function\n",
        "def f(x, A):\n",
        "    return np.exp(-np.trace(np.dot(np.dot(np.transpose(x), A), x)))\n",
        "\n",
        "# theoretical derivative (see lecture notes)\n",
        "def dfd(x, A):\n",
        "    return -f(x, A) * np.dot((np.transpose(A) + A), x)\n",
        "\n",
        "# autograd\n",
        "grad_foo = grad(f)\n",
        "\n",
        "print('Multivariate Gaussian Autogen Grad: \\n', grad_foo(x, A))\n",
        "print('Multivariate Gaussian Theoretical Grad: \\n', dfd(x, A))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "61240dc7-4960-4678-9b43-c0c15188c025",
      "metadata": {
        "id": "61240dc7-4960-4678-9b43-c0c15188c025",
        "outputId": "f90fc23a-1725-48b4-bbd1-ca703c40f38f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random w.Tx = -0.624\n",
            "ReLU Autogen Grad: \n",
            " [[0.]\n",
            " [0.]] \n",
            "\n",
            "ReLU Theoretical Grad: \n",
            " 0 \n",
            "\n",
            "\n",
            "Random w.Tx = -0.560\n",
            "ReLU Autogen Grad: \n",
            " [[0.]\n",
            " [0.]] \n",
            "\n",
            "ReLU Theoretical Grad: \n",
            " 0 \n",
            "\n",
            "\n",
            "Random w.Tx = 2.047\n",
            "ReLU Autogen Grad: \n",
            " [[1.6035526 ]\n",
            " [1.06075404]] \n",
            "\n",
            "ReLU Theoretical Grad: \n",
            " [[1.6035526 ]\n",
            " [1.06075404]] \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# rectified linear unit (ReLU)\n",
        "# it is generally better practice to name the functions something better than f\n",
        "def ReLU(w, x):\n",
        "    v = np.dot(np.transpose(w), x)\n",
        "    return np.maximum(0, v)\n",
        "\n",
        "# theoretical\n",
        "def grad_ReLU(w, x):\n",
        "    if np.dot(np.transpose(w), x) > 0:\n",
        "        return x\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# it also generally good practice to check multiple random initializations\n",
        "for i in range(3):\n",
        "    x = np.random.randn(2, 1)\n",
        "    w = np.random.randn(2, 1)\n",
        "\n",
        "    grad_foo = grad(ReLU)\n",
        "    print('Random w.Tx = %.3f'%np.dot(w.T, x)[0][0]) #this is ugly notation but I can't figure out how to make it cleaner... any thoughts?\n",
        "    print('ReLU Autogen Grad: \\n', grad_foo(w, x), '\\n')\n",
        "    print('ReLU Theoretical Grad: \\n', grad_ReLU(w, x), '\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2af41783-9c0e-48aa-8ea4-3976fee8a93b",
      "metadata": {
        "id": "2af41783-9c0e-48aa-8ea4-3976fee8a93b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
